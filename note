卷积神经网络卷积过程理解，给定输入图像如32*32*3，卷积核是10个5*5,stride=1,padding=(5-1)/2=2：
1）输入和输出。输入是32*32*3的图像，输出的特征图大小为（32+2*2-5）/1+1=32，有10个卷积核，所以输出10个32*32*1的特征图
2）卷积核大小。卷积核大小字面上是5*5，但由于输入是3通道，实际卷积的时候卷积核是5*5*通道数=5*5*3，这样可以保证无论输入是几个通道，一个卷积核产生一个二维特征图，10个卷积核可以生成10个特征图。
3）权值共享。卷积核内的数就是权值。对于每个卷积核来说，在输入32*32*3的图像上，32*32*1的特征图上的每一个神经元与输入图像有5*5*3=75个连接，也就是75个权值。如果没有权值共享的原则，那么一张特征图上32*32=1024神经元，一共产生1024*75=76800个权值，算上10个特征图的话权值数量还要乘10，运算会非常复杂。引入权值共享原则后，一张特征图上的1024个神经元共享75个权值，权值的总数就是75*10=750。可以理解为，用一个卷积核去扫图片中的每一个位置，每次卷积权重是一样的，也就是共享。进一步，一个卷积核提取输入图像的一种特征，10个卷积核自然是输出10个特征图，为了提取图像的某一种特征，整张图像当然得被同一个卷积核卷积。
>>一般规律  输入为:W1*H1*D1
           超参数（HyperParameters）:filters_num(K),filter_size(F*F),stride(S),padding(P)
           输出：W2=（W1+2*P-F）/S+1, H2=(H1+2*P-F)/S+1, D2=K
           权值总数：F*F*D1*K
4）激活函数还不是很理解。样本比一定是线性可分的，采用激活函数是为了引入非线性因素。（线性可分，用一条直线可以将数据分开）
>>怎么作用：特征图上某个神经元的数值，是通过卷积核内的参数作用在其在输入图像中的感受野得到的结果，即∑a(i,j)*w(i,j)，i,j=0:1:(F-1)。但是将这个结果赋值给这个神经元之前，需要通过激活函数，比如ReLU函数，小于0时，神经元值为0，大于0时，神经元的值才是卷积结果。
>>激活函数ReLU=max(0,x)的理解：x>0时，梯度恒为1，没有梯度耗散问题，收敛速度快。去除数据中的冗余，最大可能保留数据的特征构建稀疏矩阵。训练完成后，有许多神经元为0，提取出来的特征更具有代表性，泛化能力强。这样，达到同样的效果真正起作用的神经元数量更少，网络泛化能力更强。而且由于是稀疏矩阵，运算量变小。
  https://blog.csdn.net/yjl9122/article/details/70198357
5）池化。种类：常见均值池化AvyPooling和最大值池化MaxPooling。有重叠和不重叠之分。
>>为什么要池化？A。一般的池化层作用是，一方面缩小特征图的尺寸，简化网络的复杂度，另一方面进行特征压缩，提取主要的特征、提出次要特征。B.还有一种是空间金字塔池化SPP，稍后讲。
>>池化的合理性：图像具有一种“静态性”属性，这意味着图像一个区域的有用的特征在附近区域可能同样有用，这意味着，当处理较大尺寸图像时

池化层可以放在两种位置，一种是卷积层与卷积层之间，主要为了降低卷积层输出的特征向量的维度，改善结果、降低过拟合；另一种是卷积层和全连接层之间




